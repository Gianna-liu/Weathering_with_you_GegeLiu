{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb62241c",
   "metadata": {},
   "source": [
    "## Project part 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d6ecc",
   "metadata": {},
   "source": [
    "## Links\n",
    "- Github repository:https://github.com/Gianna-liu/IND320_dashboard_GegeLiu\n",
    "\n",
    "- Streamlit App: https://weatheringwithyou-gegeliu.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d3f43",
   "metadata": {},
   "source": [
    "## Development Log\n",
    "For Part 4 of the assignment, I felt a bit lost at the beginning because I haven’t had much experience working with time-series data. Most of my previous work involved tabular data, and later some image data. This part required developing four features, but since I had already gained some understanding of sliding-window correlation and model prediction from the lectures, I decided to start with those two components.\n",
    "\n",
    "I began by making sure I fully understood the model’s input, output workflow, and frequently referred to and debugged the example code provided in the lecture. After getting about 60% of those two tasks done, I moved on to the spatial analysis part when mainly implementing the map features such as snow drift calculation and drawing the wind rose chart. At first, I could only mimic the provided code, but as I became more familiar with it, I was able to complete the bonus task of computing monthly snow drift as well.\n",
    "\n",
    "For the time-series part specifically, selecting appropriate parameters was quite challenging, and it’s something I still need to improve on. In developing the Streamlit app, I also spent a significant amount of time designing the subpages and managing how data is passed using session_state.\n",
    "\n",
    "Once all the new features were implemented, I started refactoring the entire app. This also took quite a bit of time, as I kept discovering areas that could be optimized. Eventually, I completed a version that I’m genuinely satisfied with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40690ba",
   "metadata": {},
   "source": [
    "### Sliding window correlation between Meteorology and energy production\n",
    "\n",
    "- In January 2024, Oslo experienced extreme cold weather (temperatures dropped to -27 °C). I used a 70-hour sliding window to study the relationship between outdoor temperature and household electricity consumption. The immediate correlation between the two was already strong, but it significantly increased when the temperature lagged by 6–12 hours.\n",
    "\n",
    "- This makes sense: when the air suddenly becomes very cold, buildings cool slowly due to thermal inertia. Heating systems take several hours to start up, so there is a delay in the response of electricity consumption. Therefore, the highest correlation typically occurs with a lag of around 10 hours.\n",
    "\n",
    "- Unlike household electricity consumption, for thermal power generation, the highest correlation with temperature has almost no lag. Thermal power generation does not have a significant delayed response, and its peak correlation typically occurs with a lag of 0 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ee7be",
   "metadata": {},
   "source": [
    "### Bonus part\n",
    "\n",
    "- For the bonus task, I chose to work on calculating the monthly snow drift and plotting it together with the yearly snow drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62924b",
   "metadata": {},
   "source": [
    "## AI usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e14292",
   "metadata": {},
   "source": [
    "During this assignment, I used GitHub Copilot in my VS Code environment to assist with code review, debugging, and improving overall code structure.\n",
    "\n",
    "I asked GPT to explain some of the lecture concepts in more detail, such as the SARIMAX model. However, most of my AI usage was focused on helping with the web development process, including Streamlit UI design and styling. I also used it to explore how to optimize query performance when working with the MongoDB connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a709fd8",
   "metadata": {},
   "source": [
    "## Tasks1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c10caf",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTE_API = False  # Avoid execute api when convert into html file\n",
    "EXECUTE_localDB = False   # Avoid execute database when convert into html file\n",
    "EXECUTE_remoteDB = False \n",
    "\n",
    "import json\n",
    "import os\n",
    "from pyjstat import pyjstat\n",
    "import requests\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, to_timestamp, year, count,month\n",
    "\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8b699",
   "metadata": {},
   "source": [
    "### Step1: Load data with API and insert the data to Cassandra with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805abc8d",
   "metadata": {},
   "source": [
    "#### 1.Preparation with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f336a",
   "metadata": {},
   "source": [
    "##### Set environment variables for PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbdd1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/Cellar/openjdk@17/17.0.17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4555f27",
   "metadata": {},
   "source": [
    "##### Create a spark session to transfer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a40fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/miniconda3/envs/D2D_project/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/liugege/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/liugege/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-937d2822-40c3-4847-bc1a-97d7ea1841f8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      ":: resolution report :: resolve 325ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-937d2822-40c3-4847-bc1a-97d7ea1841f8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/8ms)\n",
      "25/11/21 16:43:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a8a977",
   "metadata": {},
   "source": [
    "#### 2.preparation in Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b69f70",
   "metadata": {},
   "source": [
    "##### Connect to the Cassandra cluster from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9bfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Cassandra\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffde73",
   "metadata": {},
   "source": [
    "##### Set up new keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e2cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXECUTE_localDB:\n",
    "    session.execute(\"CREATE KEYSPACE IF NOT EXISTS elnub WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f63649",
   "metadata": {},
   "source": [
    "##### Create a table\n",
    "- IF NOT EXISTS makes sure we do not overwrite existing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new table (first time only)\n",
    "# ##!For a composite primary key like PRIMARY KEY ((priceArea, productionGroup), startTime), you don’t need to create a separate combined column.\n",
    "# if EXECUTE_localDB:\n",
    "#     session.set_keyspace('elnub')\n",
    "#     session.execute(\"DROP TABLE IF EXISTS elnub.production_data;\")\n",
    "#     session.execute(\"CREATE TABLE IF NOT EXISTS elnub.production_data (\\\n",
    "#         priceArea TEXT,\\\n",
    "#         productionGroup TEXT,\\\n",
    "#         startTime TIMESTAMP,\\\n",
    "#         quantityKwh DOUBLE,\\\n",
    "#         endTime TIMESTAMP,\\\n",
    "#         lastUpdatedTime TIMESTAMP,\\\n",
    "#         PRIMARY KEY ((priceArea, productionGroup), startTime));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b98988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new table (first time only)\n",
    "# ###!For a composite primary key like PRIMARY KEY ((priceArea, productionGroup), startTime), you don’t need to create a separate combined column.\n",
    "# if EXECUTE_localDB:\n",
    "#     session.set_keyspace('elnub')\n",
    "#     session.execute(\"DROP TABLE IF EXISTS elnub.consumption_data;\")\n",
    "#     session.execute(\"CREATE TABLE IF NOT EXISTS elnub.consumption_data (\\\n",
    "#         priceArea TEXT,\\\n",
    "#         consumptionGroup TEXT,\\\n",
    "#         startTime TIMESTAMP,\\\n",
    "#         quantityKwh DOUBLE,\\\n",
    "#         meteringPointCount DOUBLE,\\\n",
    "#         endTime TIMESTAMP,\\\n",
    "#         lastUpdatedTime TIMESTAMP,\\\n",
    "#         PRIMARY KEY ((priceArea, consumptionGroup), startTime));\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab65a8a",
   "metadata": {},
   "source": [
    "#### 3.Retrieve data with Elhub API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1c782",
   "metadata": {},
   "source": [
    "##### Create function to enhance code reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea43dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_start_end(year, m):\n",
    "    '''\n",
    "    This function is used to generate a whole month time interval \n",
    "    corresponding to the Norwegian time zone for the specified year and month.\n",
    "    '''\n",
    "    norway_tz = pytz.timezone(\"Europe/Oslo\")\n",
    "    \n",
    "    start_date = norway_tz.localize(datetime(year, m, 1))\n",
    "    if m == 12:\n",
    "        end_date = norway_tz.localize(datetime(year + 1, 1, 1)) - timedelta(hours=1)\n",
    "    else:\n",
    "        end_date = norway_tz.localize(datetime(year, m + 1, 1)) - timedelta(hours=1)\n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38192cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parse_data_fra_api(start_dt, end_dt, required_dataset, attr_val):\n",
    "    '''\n",
    "    This function is used to obtain production data for a defined time period from the Elhub API \n",
    "    and parse the data into a list.\n",
    "    '''\n",
    "    url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "    params = {\n",
    "        \"dataset\": required_dataset,\n",
    "        \"startDate\": start_dt.isoformat(),\n",
    "        \"endDate\": end_dt.isoformat()\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get data for {start_dt} - {end_dt}: status {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    data_json = response.json() # \n",
    "    parsed_data = []\n",
    "    for data in data_json['data']:\n",
    "        for item in data['attributes'][attr_val]:\n",
    "            parsed_data.append(item)\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96679b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cassandra_ved_spark(data_list, df_col_list, table=\"production_data\", keyspace=\"elnub\"):\n",
    "    '''\n",
    "    This function writes a list of electricity production data to a local Cassandra database with spark.\n",
    "    '''\n",
    "    if not data_list:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    # define the low-case columns\n",
    "    # df.columns = ['endtime','lastupdatedtime','pricearea','productiongroup','quantitykwh','starttime']\n",
    "    df.columns = df_col_list\n",
    "    # Use spark to insert data into cassandra\n",
    "    spark.createDataFrame(df)\\\n",
    "        .write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table, keyspace=keyspace)\\\n",
    "        .mode(\"append\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99df2a",
   "metadata": {},
   "source": [
    "##### Append the production data from 2022 to 2024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 1 finished.\n",
      "Month 2 finished.\n",
      "Month 3 finished.\n",
      "Month 4 finished.\n",
      "Month 5 finished.\n",
      "Month 6 finished.\n",
      "Month 7 finished.\n",
      "Month 8 finished.\n",
      "Month 9 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n",
      "Processing year: 2023\n",
      "Month 1 finished.\n",
      "Month 2 finished.\n",
      "Month 3 finished.\n",
      "Month 4 finished.\n",
      "Month 5 finished.\n",
      "Month 6 finished.\n",
      "Month 7 finished.\n",
      "Month 8 finished.\n",
      "Month 9 finished.\n",
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n",
      "Processing year: 2024\n",
      "Month 1 finished.\n",
      "Month 2 finished.\n",
      "Month 3 finished.\n",
      "Month 4 finished.\n",
      "Month 5 finished.\n",
      "Month 6 finished.\n",
      "Month 7 finished.\n",
      "Month 8 finished.\n",
      "Month 9 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n"
     ]
    }
   ],
   "source": [
    "start_year = 2022\n",
    "end_year = 2024\n",
    "required_dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "attr_val = \"productionPerGroupMbaHour\"\n",
    "col_list = ['endtime','lastupdatedtime','pricearea','productiongroup','quantitykwh','starttime']\n",
    "\n",
    "if EXECUTE_API:\n",
    "    for one_year in range(start_year, end_year + 1):\n",
    "        print(f\"Processing year: {one_year}\")\n",
    "        for m in range(1, 13):\n",
    "            start_dt, end_dt = get_period_start_end(one_year, m)\n",
    "            # Special handling for October DST change\n",
    "            if m == 10:\n",
    "                norway_tz = pytz.timezone(\"Europe/Oslo\")\n",
    "                '''\n",
    "                The API limits retrieval to one month at a time, and October data initially failed \n",
    "                due to the Daylight Saving Time transition, which added an extra hour. \n",
    "                I resolved this by splitting October into two parts.\n",
    "                '''\n",
    "                end_first_part = norway_tz.localize(datetime(one_year, 10, 20, 23))\n",
    "                parts = [(start_dt, end_first_part),\n",
    "                        (norway_tz.localize(datetime(one_year, 10, 21, 0)),\n",
    "                        norway_tz.localize(datetime(one_year, 10, 31, 23)))]\n",
    "            else:\n",
    "                parts = [(start_dt, end_dt)]\n",
    "            \n",
    "            for start_part, end_part in parts:\n",
    "                data_list = load_parse_data_fra_api(start_part, end_part, required_dataset,attr_val)\n",
    "                write_to_cassandra_ved_spark(data_list,col_list)\n",
    "            \n",
    "            print(f\"Month {m} finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392c6a6",
   "metadata": {},
   "source": [
    "##### Load the consumption data from 2021 to 2024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b50f1611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 1 finished.\n",
      "Month 2 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 3 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 4 finished.\n",
      "Month 5 finished.\n",
      "Month 6 finished.\n",
      "Month 7 finished.\n",
      "Month 8 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 9 finished.\n",
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n",
      "Processing year: 2022\n",
      "Month 1 finished.\n",
      "Month 2 finished.\n",
      "Month 3 finished.\n",
      "Month 4 finished.\n",
      "Month 5 finished.\n",
      "Month 6 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 7 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 8 finished.\n",
      "Month 9 finished.\n",
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n",
      "Processing year: 2023\n",
      "Month 1 finished.\n",
      "Month 2 finished.\n",
      "Month 3 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 4 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 5 finished.\n",
      "Month 6 finished.\n",
      "Month 7 finished.\n",
      "Month 8 finished.\n",
      "Month 9 finished.\n",
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n",
      "Processing year: 2024\n",
      "Month 1 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2 finished.\n",
      "Month 3 finished.\n",
      "Month 4 finished.\n",
      "Month 5 finished.\n",
      "Month 6 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 7 finished.\n",
      "Month 8 finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 9 finished.\n",
      "Month 10 finished.\n",
      "Month 11 finished.\n",
      "Month 12 finished.\n"
     ]
    }
   ],
   "source": [
    "start_year = 2021\n",
    "end_year = 2024\n",
    "required_dataset = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "attr_val = \"consumptionPerGroupMbaHour\"\n",
    "col_list = ['consumptiongroup','endtime','lastupdatedtime','meteringpointcount','pricearea','quantitykwh','starttime']\n",
    "table_nm = \"consumption_data\"\n",
    "keyspace = \"elnub\"\n",
    "\n",
    "if EXECUTE_API:\n",
    "    for one_year in range(start_year, end_year + 1):\n",
    "        print(f\"Processing year: {one_year}\")\n",
    "        for m in range(1, 13):\n",
    "            start_dt, end_dt = get_period_start_end(one_year, m)\n",
    "            # Special handling for October DST change\n",
    "            if m == 10:\n",
    "                norway_tz = pytz.timezone(\"Europe/Oslo\")\n",
    "                '''\n",
    "                The API limits retrieval to one month at a time, and October data initially failed \n",
    "                due to the Daylight Saving Time transition, which added an extra hour. \n",
    "                I resolved this by splitting October into two parts.\n",
    "                '''\n",
    "                end_first_part = norway_tz.localize(datetime(one_year, 10, 20, 23))\n",
    "                parts = [(start_dt, end_first_part),\n",
    "                        (norway_tz.localize(datetime(one_year, 10, 21, 0)),\n",
    "                        norway_tz.localize(datetime(one_year, 10, 31, 23)))]\n",
    "            else:\n",
    "                parts = [(start_dt, end_dt)]\n",
    "        \n",
    "            for start_part, end_part in parts:\n",
    "                data_list = load_parse_data_fra_api(start_part, end_part, required_dataset, attr_val)\n",
    "                write_to_cassandra_ved_spark(data_list,col_list,table_nm,keyspace)\n",
    "        \n",
    "            print(f\"Month {m} finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a515a",
   "metadata": {},
   "source": [
    "### Step2: Brifly explore data with Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d77b8",
   "metadata": {},
   "source": [
    "#### load data from cassandra to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8df653ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+-------------------+-------------------+-----------+\n",
      "|pricearea|productiongroup|          starttime|            endtime|    lastupdatedtime|quantitykwh|\n",
      "+---------+---------------+-------------------+-------------------+-------------------+-----------+\n",
      "|      NO2|          hydro|2021-01-01 00:00:00|2021-01-01 01:00:00|2024-12-20 10:35:40|  7245923.5|\n",
      "|      NO2|          hydro|2021-01-01 01:00:00|2021-01-01 02:00:00|2024-12-20 10:35:40|  6750958.0|\n",
      "|      NO2|          hydro|2021-01-01 02:00:00|2021-01-01 03:00:00|2024-12-20 10:35:40|  6070989.0|\n",
      "|      NO2|          hydro|2021-01-01 03:00:00|2021-01-01 04:00:00|2024-12-20 10:35:40|  5851299.0|\n",
      "|      NO2|          hydro|2021-01-01 04:00:00|2021-01-01 05:00:00|2024-12-20 10:35:40|  5812150.0|\n",
      "+---------+---------------+-------------------+-------------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_cassandra = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"production_data\", keyspace=\"elnub\") \\\n",
    "    .load()\n",
    "\n",
    "df_from_cassandra.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7aef76",
   "metadata": {},
   "source": [
    "#### Explore the dataset briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61144fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:================================================>      (15 + 2) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+\n",
      "|year|total_quantitykwh    |row_count|\n",
      "+----+---------------------+---------+\n",
      "|2021|1.569205538468405E11 |215033   |\n",
      "|2022|1.4581949517553506E11|218675   |\n",
      "|2023|1.5381816021331845E11|218675   |\n",
      "|2024|1.570131566350712E11 |219275   |\n",
      "+----+---------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark = df_from_cassandra.select(\"pricearea\", \"productiongroup\", \"starttime\", \"quantitykwh\")\n",
    "# Add year column\n",
    "df_spark = df_spark.withColumn(\"year\",year(to_timestamp(\"starttime\")))\n",
    "df_spark.groupBy(\"year\").agg(\n",
    "    sum(\"quantitykwh\").alias(\"total_quantitykwh\"),\n",
    "    count(\"*\").alias(\"row_count\")\n",
    ").orderBy(\"year\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815f87b",
   "metadata": {},
   "source": [
    "There are not any null or duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdbd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "871658"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_spark.dropna(subset=['starttime', 'quantitykwh', 'pricearea', 'productiongroup'])\n",
    "df_clean = df_clean.dropDuplicates(['pricearea', 'productiongroup', 'starttime'])\n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c9f36",
   "metadata": {},
   "source": [
    "select the new data and drop the year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b4e1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "656625"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_new = (df_spark\n",
    "                .filter((df_spark.year >=2022) & (df_spark.year <=2024))\n",
    "                .drop(\"year\"))\n",
    "df_spark_new.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604d3a5",
   "metadata": {},
   "source": [
    "Did the same thing for the consumpition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af747931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-------------------+-------------------+-------------------+------------------+-----------+\n",
      "|pricearea|consumptiongroup|          starttime|            endtime|    lastupdatedtime|meteringpointcount|quantitykwh|\n",
      "+---------+----------------+-------------------+-------------------+-------------------+------------------+-----------+\n",
      "|      NO5|       secondary|2021-01-01 00:00:00|2021-01-01 01:00:00|2024-12-20 10:35:40|            5984.0|  1094799.6|\n",
      "|      NO5|       secondary|2021-01-01 01:00:00|2021-01-01 02:00:00|2024-12-20 10:35:40|            5984.0|  1099480.8|\n",
      "|      NO5|       secondary|2021-01-01 02:00:00|2021-01-01 03:00:00|2024-12-20 10:35:40|            5984.0|  1054455.9|\n",
      "|      NO5|       secondary|2021-01-01 03:00:00|2021-01-01 04:00:00|2024-12-20 10:35:40|            5984.0|  1049728.6|\n",
      "|      NO5|       secondary|2021-01-01 04:00:00|2021-01-01 05:00:00|2024-12-20 10:35:40|            5984.0|  1099942.9|\n",
      "+---------+----------------+-------------------+-------------------+-------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_consump_from_cassandra = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"consumption_data\", keyspace=\"elnub\") \\\n",
    "    .load()\n",
    "\n",
    "df_consump_from_cassandra.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b348e",
   "metadata": {},
   "source": [
    "The difference count between 2024 and other years because the february in 2024 has 29 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff6bfe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:=============================================>         (14 + 3) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+\n",
      "|year|total_quantitykwh    |row_count|\n",
      "+----+---------------------+---------+\n",
      "|2021|1.3145741822485603E11|218675   |\n",
      "|2022|1.258865769915383E11 |218675   |\n",
      "|2023|1.2811225028317383E11|218675   |\n",
      "|2024|1.3071573243972789E11|219275   |\n",
      "+----+---------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_consump_spark = df_consump_from_cassandra.select(\"pricearea\", \"consumptiongroup\", \"starttime\", \"quantitykwh\")\n",
    "# Add year column\n",
    "df_consump_spark = (\n",
    "    df_consump_spark\n",
    "        .withColumn(\"year\",  year(to_timestamp(\"starttime\")))\n",
    "        .withColumn(\"month\", month(to_timestamp(\"starttime\")))\n",
    ")\n",
    "df_consump_spark.groupBy(\"year\").agg(\n",
    "    sum(\"quantitykwh\").alias(\"total_quantitykwh\"),\n",
    "    count(\"*\").alias(\"row_count\")\n",
    ").orderBy(\"year\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bb68593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:======================================>                (12 + 5) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|month|row_count|\n",
      "+-----+---------+\n",
      "|1    |18575    |\n",
      "|2    |16775    |\n",
      "|3    |18550    |\n",
      "|4    |17975    |\n",
      "|5    |18575    |\n",
      "|6    |17975    |\n",
      "|7    |18575    |\n",
      "|8    |18575    |\n",
      "|9    |17975    |\n",
      "|10   |18575    |\n",
      "|11   |17975    |\n",
      "|12   |18575    |\n",
      "+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_consump_spark.filter(df_consump_spark.year ==2023).groupBy(\"month\").agg(\n",
    "    count(\"*\").alias(\"row_count\")\n",
    ").orderBy(\"month\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a311fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 185:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|month|row_count|\n",
      "+-----+---------+\n",
      "|1    |18575    |\n",
      "|2    |17375    |\n",
      "|3    |18550    |\n",
      "|4    |17975    |\n",
      "|5    |18575    |\n",
      "|6    |17975    |\n",
      "|7    |18575    |\n",
      "|8    |18575    |\n",
      "|9    |17975    |\n",
      "|10   |18575    |\n",
      "|11   |17975    |\n",
      "|12   |18575    |\n",
      "+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_consump_spark.filter(df_consump_spark.year ==2024).groupBy(\"month\").agg(\n",
    "    count(\"*\").alias(\"row_count\")\n",
    ").orderBy(\"month\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cd1122d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "875300"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consump_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9b9be",
   "metadata": {},
   "source": [
    "There are not any null or duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6212667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "875300"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consump_clean = df_consump_spark.dropna(subset=['starttime', 'quantitykwh', 'pricearea', 'consumptiongroup'])\n",
    "df_consump_clean = df_consump_clean.dropDuplicates(['pricearea', 'consumptiongroup', 'starttime'])\n",
    "df_consump_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14fb62",
   "metadata": {},
   "source": [
    "### Step3: Push data into Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ff58e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mongodb(df_spark, mongo_collection, EXECUTE_remoteDB=True):\n",
    "    if EXECUTE_remoteDB:\n",
    "        with open(\"config_local.json\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        mongo_uri = config[\"mongo_uri\"]\n",
    "\n",
    "        # Create a new client and connect to the server\n",
    "        client = MongoClient(mongo_uri, server_api=ServerApi('1'))\n",
    "        # Send a ping to confirm a successful connection\n",
    "        try:\n",
    "            client.admin.command('ping')\n",
    "            print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # connect to the Mongodb\n",
    "        db = client[\"elhub_db\"] \n",
    "        collection = db[mongo_collection] \n",
    "        df = df_spark.toPandas()\n",
    "        data_dict = df.to_dict(orient=\"records\")\n",
    "\n",
    "        # insert the data \n",
    "        collection.insert_many(data_dict)\n",
    "\n",
    "        print(f\"{len(data_dict)} records inserted into MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 20:51:32 WARN ControlConnection: [s1] Error connecting to Node(endPoint=localhost/127.0.0.1:9042, hostId=null, hashCode=6a1b1d4d), trying next node (ConnectionInitException: [s1|control|id: 0xc3181e4b, L:/127.0.0.1:62101 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (OPTIONS): unexpected failure (java.lang.IllegalArgumentException: Unsupported request opcode: 0 in protocol 127))\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656625 records inserted into MongoDB.\n"
     ]
    }
   ],
   "source": [
    "write_to_mongodb(df_spark_new,\"production_data\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69fe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875300 records inserted into MongoDB.\n"
     ]
    }
   ],
   "source": [
    "write_to_mongodb(df_consump_spark,\"consumption_data\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d86f6c",
   "metadata": {},
   "source": [
    "Close the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6337a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
